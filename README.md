# SaveTheWorld

# Abstract

Today we hear about climate change almost everyday, on the news, tv shows and social
media. It’s a topic that is urgent to address, in order to preserve the planet we’re living in and
be sure to make it habitable for further generations.
So, the goal of this project is to make people aware of how our situation is already critical
and how quickly we should act in order to reach a sustainable result. With this concept in our
mind, we thought that an interactive installation could send a clear message to the user, i.e.
that everybody can contribute to change things for the better.
The system will need historical data gathered from the web concerning the main threats of
our planet such as CO2 emission, deforestation and raising temperature.
Starting from a 3D globe, the user will be able to choose a specific area: once it is selected,
it will be displayed a 2D scenario representing its conditions and relative parameters.
The simulation will be accompanied with a visual degeneration of the planet, with effects
such as obscuration, burning or liquefying, until when we reach the 3°C increase of
temperature, with respect to the pre-industrial era.
After that, the simulation will start over, but when the user reaches the present day it will
slow down and give the possibility to get to action. Therefore, it will be possible to choose
some parameters, such as, for example, stopping or reducing emission and planting trees.
The user can operate through hand gestures that will lead to a change also of the prediction.
 In order to better have an idea on how fast we have to act to reach the target, the user will
 be able to control some values. For example, the intensity of the changing of the
 parameters, the zone where to act and the target to reach in terms of degree over time.
To have a visual representation of the world and also of the specific areas where the user
wants to operate, Touchdesigner or Processing will be used, exploiting their compatibility
with Python, the latter responsible for taking the gestures. Future scenarios, influenced by
the choices taken, will be implemented with a prediction based on RNN or Markov Chain,
then displayed. In the end, Osc and Supercollider will generate soundscapes according to
the health of a specific area’s conditions.

# Useful Links

- Generative Human Models: https://github.com/google-research/google-research/tree/master/ghum
- MediaPipe: https://google.github.io/mediapipe/solutions/pose ; https://ai.googleblog.com/2020/08/on-device-real-time-body-pose-tracking.html
- Multiperson (? for example "each person = 1 bilion people): https://www.section.io/engineering-education/multi-person-pose-estimator-with-python/
